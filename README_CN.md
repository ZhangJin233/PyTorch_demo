# PyTorch_demo

## 项目概述

这个项目演示了如何使用 PyTorch 和 MLflow 进行模型训练、跟踪和评估。通过本项目，你将学习如何使用 MLflow 跟踪实验、记录指标和参数，以及如何创建独立的评估脚本来解耦训练和测试过程。

## 环境设置

1. 创建一个项目文件夹，使用 venv 或 conda 创建虚拟环境
```bash
python -m venv .venv
source .venv/bin/activate  # 在 Windows 上使用 .venv\Scripts\activate
```

2. 安装必要的库
```bash
pip install -r requirements.txt
```

## 项目步骤

### 1. 基础训练脚本
- 加载 torchvision 自带的 FashionMNIST 数据集
- 定义一个简单的神经网络模型
- 编写训练和评估函数
- 在训练结束后，在测试集上进行评估
- 目标：先跑通一个能正常工作的基线版本

### 2. 集成 MLflow Tracking
- 改造脚本：在训练脚本中引入 mlflow
- `mlflow.start_run()`：将训练和评估代码包裹起来
- `mlflow.log_param()`：记录本次实验的超参数，例如：学习率、批量大小、训练轮次
- `mlflow.log_metric()`：在每个 epoch 结束时，记录训练损失和验证准确率
- `mlflow.log_artifact()`：记录输出产物，如混淆矩阵图和分类报告
- `mlflow.pytorch.log_model()`：将训练好的 PyTorch 模型记录下来

### 3. 实验与分析
- 在项目目录下运行 `mlflow ui`
- 进行对比实验：修改超参数（例如，尝试不同的学习率），多次运行脚本
- 在 MLflow UI 中分析：
  - 对比不同 "Run" 的参数和最终指标，找出效果最好的实验
  - 查看指标随时间变化的图表
  - 预览保存的混淆矩阵图和分类报告
- 核心体验：感受 MLflow 如何让每次模型 "测试" 都变得有据可查、可追溯、可对比

### 4. 独立评估脚本
- 项目中包含一个独立的评估脚本 `evaluate.py`，可以从 MLflow 加载训练好的模型并进行评估
- 使用方法：
  ```bash
  python evaluate.py --run-id <RUN_ID> --batch-size 64 --log-mlflow
  ```
  其中：
  - `<RUN_ID>` 是 MLflow 实验运行的 ID，可以从 MLflow UI 中获取
  - `--batch-size` 可选参数，指定评估时的批次大小
  - `--log-mlflow` 可选参数，如果提供则将评估结果记录回 MLflow
- 评估脚本会生成:
  - 详细的分类报告（classification_report.json）
  - 混淆矩阵可视化（confusion_matrix.png）
  - 错误分类样本可视化（misclassified_examples.png）
- 这种训练与评估解耦的方法是自动化测试的基础，便于 CI/CD 集成和模型的持续评估

### 5. CI/CD 集成示例
- 触发器：当代码合并到 main 分支时触发
- 执行训练：运行 pytorch_demo.py 脚本，所有结果自动记录到 MLflow Server
- 执行评估：运行 evaluate.py 脚本，加载刚刚训练出的模型
- 质量门禁：脚本判断模型的关键指标（如 Accuracy）是否大于预设阈值
- 自动晋级：如果通过门禁，将 MLflow Registry 中的模型版本从 "Staging" 更新为 "Production"
- 测试要点：这就是将模型评估融入到 CI/CD 中，实现了 AI 质量的自动化监控

## 综合分析

### 理解核心指标的含义

假设我们正在做一个多分类任务（比如 FashionMNIST 分类，共 10 个类别）。

#### 1. 损失 (Loss)

* **训练/验证损失 (`train_loss` / `validation_loss`)**:
  * **含义**: 模型预测结果与真实标签之间的"差距"。这个值越低，说明模型在对应数据集上拟合得越好。
  * **如何评估**:
    * **持续下降**: 一个健康的训练过程，损失值应该随着 epoch 的增加而稳步下降。
    * **观察拐点**: 当验证损失不再下降甚至开始上升时，通常意味着模型开始过拟合。
    * **关键在于变化趋势**: 损失的绝对值本身意义不大，关键在于它的变化趋势。

* **测试损失 (`test_loss`)**:
  * **含义**: 模型在从未见过的测试集上的损失。这是对模型泛化能力的最终度量。
  * **如何评估**: 在多个实验中，测试损失最低的那个模型通常是我们想要的候选者之一。

#### 2. 准确率 (Accuracy)

* **测试准确率 (`test_accuracy`)**:
  * **含义**: 在测试集上，模型预测正确的样本数占总样本数的比例。
  * **如何评估**:
    * **越高越好**: 我们希望准确率尽可能高。
    * **设定基线**: 需要有一个比较基准，如随机猜测基线、业务基线或业界基线。
    * **警惕数据不平衡**: 当数据类别分布不均匀时，单纯的准确率可能具有误导性。

#### 3. 精确率、召回率、F1 分数

这些指标能帮你深入分析模型在每个类别上的表现，尤其是在数据不平衡时至关重要。

* **精确率 (Precision)**:
  * **含义**: 在所有被模型预测为某类的样本中，有多少是真的属于该类。
  * **公式**: `TP / (TP + FP)`
  * **业务场景**: 在垃圾邮件检测中，你希望高精确率。你不希望将重要邮件误分类为垃圾邮件。

* **召回率 (Recall)**:
  * **含义**: 在所有真实属于某类的样本中，有多少被模型成功识别。
  * **公式**: `TP / (TP + FN)`
  * **业务场景**: 在医疗诊断中，你希望高召回率。你不希望漏掉任何一个真实患者。

* **F1 分数**:
  * **含义**: 精确率和召回率的调和平均值，提供一个综合衡量标准。
  * **公式**: `2 * (Precision * Recall) / (Precision + Recall)`
  * **如何评估**: 当你希望精确率和召回率都表现良好时，F1 分数是一个很好的单一评估指标。
